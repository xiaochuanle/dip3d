{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d7e4d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:34.702779Z",
     "start_time": "2023-08-25T03:49:33.157438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datatable as dt\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_color_codes(\"pastel\") # 颜色设定\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats import multitest\n",
    "import scipy.stats as stats\n",
    "import pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e090f542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:34.713007Z",
     "start_time": "2023-08-25T03:49:34.707567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Rawdir=\"./figure_processing_data/Fig6\"\n",
    "PATH = f\"{Rawdir}/pairwise_contact/fhap_list\"\n",
    "chr_list = [\n",
    "    \"chr1\", \"chr2\", \"chr3\", \"chr4\", \"chr5\", \"chr6\",\n",
    "    \"chr7\", \"chr8\", \"chr9\", \"chr10\", \"chr11\", \"chr12\",\n",
    "    \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n",
    "    \"chr19\", \"chr20\", \"chr21\", \"chr22\", \"chrX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece4a6e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:34.874488Z",
     "start_time": "2023-08-25T03:49:34.864163Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_fhap(path, file):\n",
    "    ### laoding phasing fragments\n",
    "    fhap_list=path + '/' + 'frag-hap_mapq5.list'\n",
    "    fhap=pd.read_csv(fhap_list,header=None,sep='\\t',names=['rID','fID','chrom','pos','hp'],\n",
    "                    usecols = [0,1,2,3,5])\n",
    "    fhap[\"chrom\"] = file\n",
    "    print(len(fhap))\n",
    "\n",
    "    ### read-level phasing stats\n",
    "    read_df = fhap.groupby('rID')['hp'].apply(list).reset_index()\n",
    "    read_df['h1'] = read_df.hp.apply(lambda x: x.count(1) if 1 in x else 0)\n",
    "    read_df['h2'] = read_df.hp.apply(lambda x: x.count(2) if 2 in x else 0)\n",
    "    read_df['un'] = read_df.hp.apply(lambda x: x.count(0) if 0 in x else 0)\n",
    "    read_df['hp'] = read_df['h1'] + read_df['h2']\n",
    "    read_df['Fc'] = read_df['hp'] + read_df['un']\n",
    "    read_df['mod_fc'] = read_df.Fc.apply(lambda x: x if x<=10 else 10)\n",
    "    read_df['ratio'] = read_df.apply(lambda x: round(x.hp/x.Fc*100,2),axis=1)\n",
    "    ##### read haplotype assign\n",
    "    \n",
    "    read_df[\"HP\"] = -1\n",
    "    P1 = read_df.h1 == read_df.hp\n",
    "    P2 = read_df.h2 == read_df.hp\n",
    "    P3 = read_df.ratio == 0\n",
    "    read_df.loc[P1, \"HP\"] = 1\n",
    "    read_df.loc[P2, \"HP\"] = 2\n",
    "    read_df.loc[P3, \"HP\"] = 0\n",
    "    Fragmentcount = read_df.set_index(\"rID\").to_dict()['Fc']\n",
    "    \n",
    "    return read_df, fhap, Fragmentcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6629f7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:35.499865Z",
     "start_time": "2023-08-25T03:49:35.490619Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_hp(read_df, fhap, filter_Frag, flanksize, binsize, Fragmentcount):\n",
    "    # Filter hp reads\n",
    "    h1_reads = read_df.loc[read_df.HP==1, \"rID\"].values\n",
    "    h2_reads = read_df.loc[read_df.HP==2, \"rID\"].values\n",
    "    fhap[\"HP\"] = 0\n",
    "    fhap.loc[fhap.rID.isin(h1_reads), \"HP\"] = 1\n",
    "    fhap.loc[fhap.rID.isin(h2_reads), \"HP\"] = 2\n",
    "    fhap_filter = fhap.loc[fhap.HP!=0, :].copy()\n",
    "    freads = read_df.loc[read_df.Fc >= filter_Frag, \"rID\"].unique()\n",
    "    fhap_filter = fhap_filter.loc[ fhap_filter.rID.isin(freads), :]\n",
    "    \n",
    "    fhap_filter[\"start\"] = fhap_filter[\"pos\"] - flanksize\n",
    "    fhap_filter[\"end\"] = fhap_filter[\"pos\"] + flanksize\n",
    "    fhap_filter[\"bin\"] = fhap_filter[\"pos\"].values // binsize\n",
    "    fhap_filter[\"Fc\"] = fhap_filter[\"rID\"].apply(lambda x: Fragmentcount[x] )\n",
    "    fhap_filter = fhap_filter.loc[:, [\"chrom\", \"start\", \"end\", \"rID\", \"fID\", \"Fc\", \"bin\", \"HP\"]]\n",
    "    \n",
    "    return fhap_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e67111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:35.738484Z",
     "start_time": "2023-08-25T03:49:35.723490Z"
    }
   },
   "outputs": [],
   "source": [
    "def FilterDF(VDF_DF, region, filter_Frag):\n",
    "    ## region filter\n",
    "    print(\"Befor filter : %d\"%len(VDF_DF))\n",
    "    Pchr = VDF_DF.chrom == region[0]\n",
    "    Pregion = (VDF_DF.start >= region[1]) & (VDF_DF.end <= region[2])\n",
    "    VDF_filter = VDF_DF.loc[ Pchr & Pregion , :]\n",
    "    print(\"After Region Filter: %d\"%len(VDF_filter) )\n",
    "    ## fragment filter\n",
    "    #Fragmentcount = VDF_filter.groupby(by=\"read_name\", as_index=True)[\"chrom\"].count()\n",
    "    Fragmentcount = {}\n",
    "    rN = 0\n",
    "    fIDlist = []\n",
    "    for  n, rowval in VDF_filter.iterrows():\n",
    "        read_name = rowval[\"read_name\"]\n",
    "        if read_name not in Fragmentcount :\n",
    "            #rN = shorten_readname(read_name) # short readID\n",
    "            rN = read_name\n",
    "            fc = 1\n",
    "            fN = 0\n",
    "        else:\n",
    "            rN, fc = Fragmentcount[read_name]\n",
    "            fc += 1\n",
    "        Fragmentcount[read_name] = (rN, fc)\n",
    "        fIDlist.append(fN)\n",
    "        fN += 1\n",
    "    ### fragment ID\n",
    "    Fragment_df = pd.DataFrame(Fragmentcount).T\n",
    "    Fragment_df.columns = [\"rID\", \"count\"]\n",
    "    VDF_filter = VDF_filter.set_index(\"read_name\")\n",
    "    VDF_filter[\"Fc\"] = 0\n",
    "    VDF_filter.loc[:, \"Fc\"] = Fragment_df.loc[VDF_filter.index, \"count\"]\n",
    "    VDF_filter.loc[:, \"rID\"] = Fragment_df.loc[VDF_filter.index, \"rID\"]\n",
    "    VDF_filter[\"fID\"] = fIDlist\n",
    "    VDF_filter = VDF_filter.loc[ VDF_filter.Fc >= filter_Frag, :] \n",
    "    VDF_filter = VDF_filter.reset_index(drop=True)\n",
    "    print(\"After Fragment number Filter: %d reads\"%len( set(VDF_filter.rID.values) ) )\n",
    "    return (VDF_filter)\n",
    "\n",
    "# Bins \n",
    "def BinsDF(df, binsize=1000):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"pos\"] = (df.start.values + df.end.values)/2\n",
    "    df[\"pos\"] = df[\"pos\"].astype(\"int\")\n",
    "    df[\"bin\"] =  ( df[\"pos\"].values/binsize ).astype(\"int\")\n",
    "    #df = df.drop([\"start\", \"end\", \"Fragnum\"], axis=1)\n",
    "    return (df)\n",
    "\n",
    "# Loading\n",
    "def Loading(filename, region, filter_Frag, binsize = 1000):\n",
    "    '''\n",
    "    paf Loading\n",
    "    '''\n",
    "    print(\"Loading %s\"%filename)\n",
    "    usecols = [0,5,7,8]\n",
    "    colnames = ['read_name', 'chrom', 'start', 'end']\n",
    "    VDF_DF = LoadTables(filename, \"\\t\",  usecols, colnames)\n",
    "    VDF_filter = FilterDF(VDF_DF, region, filter_Frag)\n",
    "    del(VDF_DF)\n",
    "    gc.collect()\n",
    "    # Bin calculate\n",
    "    bin_df = BinsDF(VDF_filter, binsize)\n",
    "    bin_df = bin_df.loc[:, [\"chrom\", \"start\", \"end\", \"rID\", \"fID\", \"Fc\", \"bin\"]]\n",
    "    del(VDF_filter)\n",
    "    return(bin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2161e857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:36.130993Z",
     "start_time": "2023-08-25T03:49:36.118055Z"
    }
   },
   "outputs": [],
   "source": [
    "from pybedtools import BedTool\n",
    "\n",
    "def LoadingEnhancerbed(filename, region, binsize=1000, disthred = 5000, flanklen=0):\n",
    "    print(\"Loading %s\"%filename)\n",
    "    input_bed = BedTool(filename)\n",
    "    merged_bed = input_bed.sort().merge( d = disthred) # merge the region within disthred\n",
    "    merged_bed.saveas('/tmp/merged.bed')\n",
    "    bedDF = pd.read_csv( '/tmp/merged.bed', sep=\"\\t\", header=None,  names=[\"chrom\", \"start\", \"end\"], usecols=[0,1,2] )\n",
    "    # enhancer flank : if enhancer length < flanklen, than change the enhancer to the size of flanklen\n",
    "    if flanklen > 0:\n",
    "        Pos = (bedDF.end.values + bedDF.start.values) // 2\n",
    "        starts1, ends1 = Pos - flanklen//2, Pos + flanklen//2\n",
    "        P = (bedDF.end.values - bedDF.start.values) < flanklen\n",
    "        bedDF.loc[P, \"start\"] = starts1[P]\n",
    "        bedDF.loc[P, \"end\"] = ends1[P]\n",
    "    # region filter\n",
    "    Pchr = bedDF.chrom == region[0]\n",
    "    Pregion = (bedDF.start >= region[1]) & (bedDF.end <= region[2])\n",
    "    bedDF = bedDF.loc[Pchr&Pregion,:]\n",
    "    bedDF = BinsDF(bedDF, binsize)\n",
    "    bedDF[\"ID\"] = \"en\"\n",
    "    bedDF[\"ID\"] = bedDF[\"ID\"].str.cat(bedDF.index.astype(\"str\") )\n",
    "    bedDF = bedDF.loc[:, [\"chrom\", \"start\", \"end\", \"ID\", \"bin\"] ]\n",
    "    bedDF = bedDF.loc[:, [\"chrom\", \"start\", \"end\", \"ID\", \"bin\"] ]\n",
    "    return(bedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263a2d19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:36.767671Z",
     "start_time": "2023-08-25T03:49:36.759516Z"
    }
   },
   "outputs": [],
   "source": [
    "def LoadingPro(filename, region, binsize):\n",
    "    print(\"Loading %s\"%filename)\n",
    "    bedDF = pd.read_csv(filename, sep=\"\\t\", header=None, \n",
    "                        usecols=[0,1,2,6], names=[\"chrom\",\"start\",\"end\",\"ID\"])\n",
    "    Pchr = bedDF.chrom == region[0]\n",
    "    Pregion = (bedDF.start >= region[1]) & (bedDF.end <= region[2])\n",
    "    bedDF = bedDF.loc[Pchr&Pregion,:]\n",
    "    bedDF = BinsDF(bedDF, binsize)\n",
    "    bedDF = bedDF.loc[:, [\"chrom\", \"start\", \"end\", \"ID\", \"bin\"]]\n",
    "    return(bedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff24e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:37.314851Z",
     "start_time": "2023-08-25T03:49:37.305471Z"
    }
   },
   "outputs": [],
   "source": [
    "def intersection(pro_bed, en_bed, binVDF):\n",
    "    \n",
    "    pbed = BedTool.from_dataframe(pro_bed)\n",
    "    ebed = BedTool.from_dataframe(en_bed)\n",
    "    fbed = BedTool.from_dataframe(binVDF)\n",
    "\n",
    "    intersect = fbed.intersect(pbed, wa=True, wb=True, loj=True)\n",
    "    intersect2 = intersect.intersect(ebed, wa=True, wb=True, loj=True)\n",
    "\n",
    "    colnames = [\"chrom\", \"start\", \"end\", \"rID\", \"fID\",\"Fc\", \"bin\",\"HP\",\n",
    "                \"Pchrom\", \"Pstart\", \"Pend\", \"PID\", \"Pbin\", \"Ppos\",\n",
    "               \"Echrom\", \"Estart\", \"Eend\", \"EID\", \"Ebin\"]\n",
    "    int_df = intersect2.to_dataframe(names=colnames)\n",
    "    int_df = int_df.drop([\"Pchrom\", \"Pstart\", \"Pend\", \"Pbin\", \"Ppos\",\"Echrom\", \"Estart\", \"Eend\", \"Ebin\"], axis=1)\n",
    "    # 清理临时文件\n",
    "    pybedtools.cleanup() \n",
    "    # write the output to a file\n",
    "    return int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a3a7e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:37.691061Z",
     "start_time": "2023-08-25T03:49:37.676499Z"
    }
   },
   "outputs": [],
   "source": [
    "def Esummary(pro_bed, int_df):\n",
    "    Pro_dict = pro_bed.set_index('ID').to_dict()[\"pos\"]\n",
    "    Promoters = int_df.PID.unique()\n",
    "    rIDs = int_df.loc[int_df.PID !=\".\", \"rID\"].unique()\n",
    "    df = int_df.loc[int_df.rID.isin(rIDs), :].reset_index()\n",
    "    ## Enhancer types\n",
    "    Enhancer_list = []\n",
    "    for rID, gdf in df.groupby(\"rID\"):\n",
    "        gdf = gdf.reset_index()\n",
    "        HP = gdf.HP.values[0]\n",
    "        for PID in gdf.PID.unique():\n",
    "            if PID != \".\" :\n",
    "                P = gdf.PID == PID\n",
    "                ## 非promoter fragment 去重\n",
    "                pgdf = gdf.loc[(~P), :].drop_duplicates(subset=['rID', 'fID'], keep='first').reset_index(drop=True).copy()\n",
    "                EIDs = pgdf.loc[pgdf.EID!=\".\", \"EID\"].unique() # 非promoter fragment中包含Enhancer IDs\n",
    "                Ecount = len(EIDs)\n",
    "                EIDs = \",\".join( sorted( list(EIDs) ) )\n",
    "                if Ecount <= 0:\n",
    "                    Etype = \"NE\"\n",
    "                elif Ecount == 1:\n",
    "                    Etype = \"SE\"\n",
    "                else:\n",
    "                    Etype = \"ME\"\n",
    "                item = (rID, HP, PID, EIDs, Ecount,  Etype)\n",
    "                Enhancer_list.append( item )\n",
    "\n",
    "    Ereads_df = pd.DataFrame( Enhancer_list, columns = [\"rID\", \"HP\", \"PID\", \"EIDs\", \"Ecount\", \"EType\"])\n",
    "    ## Ehancer contact model summary\n",
    "    Esummary_df = Ereads_df.groupby([\"PID\", \"HP\", \"EType\"])[\"rID\"].count().reset_index()\n",
    "    \n",
    "    return Esummary_df, Ereads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e8e564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:38.510120Z",
     "start_time": "2023-08-25T03:49:38.489875Z"
    }
   },
   "outputs": [],
   "source": [
    "def pivot_df_make(Esummary_df, Ereads_df):\n",
    "    '''\n",
    "    Calculate the ME ratio\n",
    "    '''\n",
    "    pivot_df = pd.pivot_table(Esummary_df, values='rID', \n",
    "                              index='PID', columns=['HP', \"EType\"], aggfunc='sum', fill_value=0 )\n",
    "\n",
    "    pivot_df[\"1_SE_ratio\"] =  pivot_df.loc[:, (1, \"SE\")].values / pivot_df.loc[:, [(1, \"ME\"), (1, \"SE\"), (1, \"NE\")]].sum(axis=1)\n",
    "    pivot_df[\"1_ME_ratio\"] =  pivot_df.loc[:, (1, \"ME\")].values / pivot_df.loc[:, [(1, \"ME\"), (1, \"SE\"), (1, \"NE\")]].sum(axis=1)\n",
    "    pivot_df[\"2_SE_ratio\"] =  pivot_df.loc[:, (2, \"SE\")].values / pivot_df.loc[:, [(2, \"ME\"), (2, \"SE\"), (2, \"NE\")]].sum(axis=1)\n",
    "    pivot_df[\"2_ME_ratio\"] =  pivot_df.loc[:, (2, \"ME\")].values / pivot_df.loc[:, [(2, \"ME\"), (2, \"SE\"), (2, \"NE\")]].sum(axis=1)\n",
    "    pivot_df[\"h1_Ecount_mean\"] = 0.0\n",
    "    pivot_df[\"h2_Ecount_mean\"] = 0.0 \n",
    "    \n",
    "    for ProID in pivot_df.index:\n",
    "        h1count = Ereads_df.loc[(Ereads_df.PID==ProID)&(Ereads_df.HP==1), \"Ecount\"].to_list()\n",
    "        if len(h1count) != 0:\n",
    "            pivot_df.loc[ProID, \"h1_Ecount_mean\"] = sum(h1count)/len(h1count)\n",
    "        elif len(h1count) == 0:\n",
    "            h1count = 0\n",
    "            pivot_df.loc[ProID, \"h1_Ecount_mean\"] = 0\n",
    "        h2count = Ereads_df.loc[(Ereads_df.PID==ProID)&(Ereads_df.HP==2), \"Ecount\"].to_list()\n",
    "        if len(h2count) != 0:\n",
    "            pivot_df.loc[ProID, \"h2_Ecount_mean\"] = sum(h2count)/len(h2count)\n",
    "        elif len(h2count) == 0:\n",
    "            h2count = 0\n",
    "            pivot_df.loc[ProID, \"h2_Ecount_mean\"] = 0\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad7d8166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:49:39.752243Z",
     "start_time": "2023-08-25T03:49:39.739687Z"
    }
   },
   "outputs": [],
   "source": [
    "chrom_size = pd.read_csv(f\"{Rawdir}/pre_data/hg38.chromosomes.size\",\n",
    "                        sep = \"\\t\", header = None,\n",
    "                        names = ['chrom', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1e643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T04:07:33.082462Z",
     "start_time": "2023-08-04T11:11:44.785058Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPATH = f\"{Rawdir}/MER\"\n",
    "for file in chr_list:\n",
    "    SIZE = chrom_size.loc[chrom_size.chrom == file, 'size'].tolist()[0]\n",
    "    region = [file, 0, SIZE]\n",
    "    binsize = 5000\n",
    "    flanksize = 2500\n",
    "    promoterfile = f\"{Rawdir}/pre_data/TSS_sorted.txt\"\n",
    "    enhancerfile = f\"{Rawdir}/pre_data/GM12878_pELS.txt\"\n",
    "    pro_bed =  LoadingPro(promoterfile, region, binsize)\n",
    "    pro_bed[\"pos\"] = ( pro_bed.start.values +  pro_bed.end.values ) // 2\n",
    "    en_bed = LoadingEnhancerbed(enhancerfile, region, 1000, 5000,5000)\n",
    "    \n",
    "    path = PATH + '/' +file\n",
    "    print(f\"Loading {path}\")\n",
    "    read_df, fhap, Fragmentcount = load_fhap(path, file)\n",
    "    binVDF = filter_hp(read_df, fhap, 5, flanksize, binsize, Fragmentcount)\n",
    "    int_df = intersection(pro_bed, en_bed, binVDF)\n",
    "    Esummary_df, Ereads_df = Esummary(pro_bed, int_df)\n",
    "    pivot_df = pivot_df_make(Esummary_df, Ereads_df)\n",
    "    \n",
    "    print(f\"writing to {OUTPATH}/{file}/MER_all.txt\")\n",
    "    pivot_df.to_csv(OUTPATH + '/' + file + '/' + 'MER_all.txt',\n",
    "                     sep = \"\\t\")\n",
    "    \n",
    "    read_df = None\n",
    "    fhap = None\n",
    "    pro_bed = None\n",
    "    en_bed = None\n",
    "    binVDF = None\n",
    "    int_df = None\n",
    "    Esummary_df = None\n",
    "    pivot_df = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
