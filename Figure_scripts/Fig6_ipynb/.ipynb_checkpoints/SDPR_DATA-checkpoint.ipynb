{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8345f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:48.809306Z",
     "start_time": "2023-08-25T03:12:47.465271Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib_venn import venn2\n",
    "import re\n",
    "import warnings\n",
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efc859c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:49.168670Z",
     "start_time": "2023-08-25T03:12:49.162904Z"
    }
   },
   "outputs": [],
   "source": [
    "chr_list = [\n",
    "    \"chr1\", \"chr2\", \"chr3\", \"chr4\", \"chr5\", \"chr6\",\n",
    "    \"chr7\", \"chr8\", \"chr9\", \"chr10\", \"chr11\", \"chr12\",\n",
    "    \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n",
    "    \"chr19\", \"chr20\", \"chr21\", \"chr22\", \"chrX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0062fabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:49.386747Z",
     "start_time": "2023-08-25T03:12:49.374508Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_fhap(path):\n",
    "    \"\"\"laoding phasing fragments\"\"\"\n",
    "    fhap_list=path + '/' + 'frag-hap_mapq5.list'\n",
    "    fhap=pd.read_csv(fhap_list,header=None,sep='\\t',names=['rid','fid','cid','pos','hp'],\n",
    "                    usecols = [0,1,2,3,5])\n",
    "    print(len(fhap))\n",
    "\n",
    "    ### read-level phasing stats\n",
    "    read_df = fhap.groupby('rid')['hp'].apply(list).reset_index()\n",
    "    read_df['h1'] = read_df.hp.apply(lambda x: x.count(1) if 1 in x else 0)\n",
    "    read_df['h2'] = read_df.hp.apply(lambda x: x.count(2) if 2 in x else 0)\n",
    "    read_df['un'] = read_df.hp.apply(lambda x: x.count(0) if 0 in x else 0)\n",
    "    read_df['hp'] = read_df['h1'] + read_df['h2']\n",
    "    read_df['fc'] = read_df['hp'] + read_df['un']\n",
    "    read_df['mod_fc'] = read_df.fc.apply(lambda x: x if x<=10 else 10)\n",
    "    read_df['ratio'] = read_df.apply(lambda x: round(x.hp/x.fc*100,2),axis=1)\n",
    "    ##### read haplotype assign\n",
    "    read_df[\"HType\"] = \"h-trans\"\n",
    "    P1 = read_df.h1 == read_df.hp\n",
    "    P2 = read_df.h2 == read_df.hp\n",
    "    P3 = read_df.ratio == 0\n",
    "    read_df.loc[P1, \"HType\"] = \"h1\"\n",
    "    read_df.loc[P2, \"HType\"] = \"h2\"\n",
    "    read_df.loc[P3, \"HType\"] = \"unknown\"\n",
    "    read_df = read_df.loc[read_df.fc >= 5]\n",
    "    \n",
    "    return read_df, fhap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe92ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:49.548104Z",
     "start_time": "2023-08-25T03:12:49.541699Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_hp(read_df, fhap, FLANK):\n",
    "    \"\"\"filter reads with haplotag h1 or h2, and set the bin size to 5kb\"\"\"\n",
    "    flanksize = FLANK\n",
    "    # Filter hp reads\n",
    "    h1_reads = read_df.loc[read_df.HType==\"h1\", \"rid\"].values\n",
    "    h2_reads = read_df.loc[read_df.HType==\"h2\", \"rid\"].values\n",
    "    fhap[\"HType\"] = \"unknown\"\n",
    "    fhap.loc[fhap.rid.isin(h1_reads), \"HType\"] = \"h1\"\n",
    "    fhap.loc[fhap.rid.isin(h2_reads), \"HType\"] = \"h2\"\n",
    "    fhap_filter = fhap.loc[fhap.HType!=\"unknown\", :].copy()\n",
    "    # fragment flank as 5kb length\n",
    "    fhap_filter[\"start\"] = fhap_filter[\"pos\"] - flanksize\n",
    "    fhap_filter[\"end\"] = fhap_filter[\"pos\"] + flanksize\n",
    "    \n",
    "    return fhap_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0c859b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:49.716404Z",
     "start_time": "2023-08-25T03:12:49.709825Z"
    }
   },
   "outputs": [],
   "source": [
    "def LoadingPro(filename, region):#, binsize\n",
    "    print(\"Loading %s\"%filename)\n",
    "    bedDF = pd.read_csv(filename, sep=\"\\t\", header=None, \n",
    "                        usecols=[0,1,2,6], names=[\"Pchrom\",\"Pstart\",\"Pend\",\"ID\"])\n",
    "    Pchr = bedDF.Pchrom == region[0]\n",
    "    Pregion = (bedDF.Pstart >= region[1]) & (bedDF.Pend <= region[2])\n",
    "    bedDF = bedDF.loc[Pchr&Pregion,:]\n",
    "    return(bedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69e2248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:49.876216Z",
     "start_time": "2023-08-25T03:12:49.869894Z"
    }
   },
   "outputs": [],
   "source": [
    "def inter(pbed, fhap_filter):\n",
    "    \"\"\"pro, frag intersect\"\"\"\n",
    "    \n",
    "    pbed = BedTool.from_dataframe(pro_bed)\n",
    "    fbed = BedTool.from_dataframe(fhap_filter)\n",
    "\n",
    "    intersect = fbed.intersect(pbed, wa=True, wb=True, loj=True)\n",
    "    # intersect2 = intersect.intersect(fbed, wa=True, wb=True, loj=True)\n",
    "\n",
    "    colnames = [\"chrom\", \"start\", \"end\", \"rid\", \"fid\",\"cid\", \"pos\", \"hp\",\"HType\",\n",
    "                \"Pchrom\", \"Pstart\", \"Pend\", \"ID\"]\n",
    "    int_df = intersect.to_dataframe(names=colnames)\n",
    "\n",
    "    pybedtools.cleanup() \n",
    "    \n",
    "    \n",
    "    return int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85667ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.059190Z",
     "start_time": "2023-08-25T03:12:50.046564Z"
    }
   },
   "outputs": [],
   "source": [
    "def func(ID, df):\n",
    "    \"\"\"contact matrix make processing\"\"\"\n",
    "\n",
    "    rid_list = list(df['rid'].unique())\n",
    "    tmp_gene_list = []\n",
    "    for rid in rid_list:\n",
    "        tmp_df = rid_dict[rid]\n",
    "        tmp_df['Pchrom'] = df.loc[0, 'Pchrom']\n",
    "        tmp_df['Pstart'] = df.loc[0, 'Pstart']\n",
    "        tmp_df['Pend'] = df.loc[0, 'Pend']\n",
    "        tmp_df['ID'] = df.loc[0, 'ID']\n",
    "        tmp_gene_list.append(tmp_df)\n",
    "\n",
    "    return pd.concat(tmp_gene_list)\n",
    "import multiprocessing\n",
    "def func1_wrapper(args):\n",
    "    \"\"\"pass parameter\"\"\"\n",
    "    return func(*args)\n",
    "def file_tackle(df):\n",
    "    \"\"\"make DF\"\"\"\n",
    "    print(\"make DF\")\n",
    "    \n",
    "#     Pid_rid_group = int_df_filter.groupby(['rid'])\n",
    "#     Pid_rid_dict = {}\n",
    "#     for group, df in Pid_rid_group:\n",
    "#         Pid_rid_dict[group] = df\n",
    "\n",
    "    \n",
    "    \n",
    "    gene_group = int_df.groupby(\"ID\")\n",
    "    gene_dict = {}\n",
    "    for ID, df in gene_group:\n",
    "        gene_dict[ID] = df.reset_index(drop = True)\n",
    "    del gene_dict['.']\n",
    "    if __name__ == '__main__':\n",
    "        with multiprocessing.Pool(processes=20) as pool:\n",
    "            args_list = [(key, value) for key, value in gene_dict.items()]\n",
    "            result = pool.map(func1_wrapper, args_list)\n",
    "    DF = pd.concat(result)\n",
    "    DF[\"fpos\"] = ( DF['start'] + DF['end'] ) / 2\n",
    "    DF[\"ppos\"] = ( DF['Pstart'] + DF['Pend'] ) / 2\n",
    "    DF[\"dis\"] = abs(( DF['ppos'] - DF['fpos'] ))\n",
    "    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10dd6be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.251633Z",
     "start_time": "2023-08-25T03:12:50.238134Z"
    }
   },
   "outputs": [],
   "source": [
    "def dis_loc_count(pid, df):\n",
    "    \"\"\"Cal DLR\"\"\"\n",
    "    concat_list = []\n",
    "    for read_id, df in df.groupby(by = \"rid\"):\n",
    "        \n",
    "        tmp_list = []\n",
    "        tmp_h1 = df.loc[df.HType == \"h1\"]\n",
    "        distal1 = len(tmp_h1.loc[tmp_h1.dis >= 1500000])\n",
    "        local1 = len(tmp_h1.loc[tmp_h1.dis < 1500000])\n",
    "\n",
    "        tmp_h2 = df.loc[df.HType == \"h2\"]\n",
    "        distal2 = len(tmp_h2.loc[tmp_h2.dis >= 1500000])\n",
    "        local2 = len(tmp_h2.loc[tmp_h2.dis < 1500000])\n",
    "\n",
    "        tmp_list.append(pid)\n",
    "        tmp_list.append(read_id)\n",
    "        tmp_list.append(distal1)\n",
    "        tmp_list.append(local1)\n",
    "        tmp_list.append(distal2)\n",
    "        tmp_list.append(local2)\n",
    "        concat_list.append(pd.DataFrame(tmp_list).T)\n",
    "    concat_df = pd.concat(concat_list).rename(columns =\n",
    "                                              {0:\"Pid\",\n",
    "                                               1:\"read_id\",\n",
    "                                               2:\"dis_h1\",\n",
    "                                               3:\"loc_h1\",\n",
    "                                               4:\"dis_h2\",\n",
    "                                               5:\"loc_h2\"}).set_index([\"Pid\",\"read_id\"])\n",
    "        \n",
    "    return concat_df\n",
    "import multiprocessing\n",
    "def func2_wrapper(args):\n",
    "    \"\"\"pass parameter\"\"\"\n",
    "    return dis_loc_count(*args)\n",
    "def computing_dlr(DF):\n",
    "    \"\"\"calculate the singel melecular Distal and Primary ratio\"\"\"\n",
    "    final_dict = {}\n",
    "    DF_group = DF.groupby(\"ID\")\n",
    "    for pid, df in DF_group:\n",
    "        final_dict[pid] = df\n",
    "    print(\"Calculating ...\")\n",
    "    if __name__ == '__main__':\n",
    "        with multiprocessing.Pool(processes=20) as pool:\n",
    "            args_list = [(key, value) for key, value in final_dict.items()]\n",
    "            result = pool.map(func2_wrapper, args_list)\n",
    "    DF_statistic = pd.concat(result)\n",
    "    return DF_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a307a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.408682Z",
     "start_time": "2023-08-25T03:12:50.400796Z"
    }
   },
   "outputs": [],
   "source": [
    "def math_div_plus(DF_result_copy):\n",
    "    \"\"\"h1_DLR\"\"\"\n",
    "    a1 = \"dis_h1\"\n",
    "    b1 = \"loc_h1\"\n",
    "    c1 = \"DLR_h1\"\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a1}'] == 0) & (DF_result_copy[f'{b1}'] ==0), f\"{c1}\"] = np.nan\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a1}'] != 0) & (DF_result_copy[f'{b1}'] !=0), f\"{c1}\"] = ((DF_result_copy.loc[(DF_result_copy[f'{a1}'] != 0) &\n",
    "                                                                                                                       (DF_result_copy[f'{b1}'] !=0), f\"{a1}\"]) /\n",
    "                                                                                                  (DF_result_copy.loc[(DF_result_copy[f'{a1}'] != 0) &\n",
    "                                                                                                                      (DF_result_copy[f'{b1}'] !=0), f\"{b1}\"]))\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a1}'] != 0) & (DF_result_copy[f'{b1}'] ==0), f\"{c1}\"] = DF_result_copy.loc[(DF_result_copy[f'{a1}'] != 0) &\n",
    "                                                                                                                     (DF_result_copy[f'{b1}'] ==0), f\"{a1}\"]\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a1}'] == 0) & (DF_result_copy[f'{b1}'] !=0), f\"{c1}\"] = 0\n",
    "    return DF_result_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec03eed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.577657Z",
     "start_time": "2023-08-25T03:12:50.568234Z"
    }
   },
   "outputs": [],
   "source": [
    "def math_div_negative(DF_result_copy):\n",
    "    \"\"\"h2_DLR\"\"\"\n",
    "    a2 = \"dis_h2\"\n",
    "    b2 = \"loc_h2\"\n",
    "    c2 = \"DLR_h2\"\n",
    "    \n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a2}'] == 0) \n",
    "                       & (DF_result_copy[f'{b2}'] ==0), f\"{c2}\"] = np.nan\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a2}'] != 0) \n",
    "                       & (DF_result_copy[f'{b2}'] !=0), f\"{c2}\"] = ((DF_result_copy.loc[(DF_result_copy[f'{a2}'] != 0) \n",
    "                                                                                        & (DF_result_copy[f'{b2}'] !=0), f\"{a2}\"]) \n",
    "                                                                    /(DF_result_copy.loc[(DF_result_copy[f'{a2}'] != 0) \n",
    "                                                                                        & (DF_result_copy[f'{b2}'] !=0), f\"{b2}\"]))\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a2}'] != 0)\n",
    "                       &(DF_result_copy[f'{b2}'] ==0), f\"{c2}\"] = DF_result_copy.loc[(DF_result_copy[f'{a2}'] != 0)\n",
    "                                                                                     & (DF_result_copy[f'{b2}'] ==0), f\"{a2}\"]\n",
    "\n",
    "    DF_result_copy.loc[(DF_result_copy[f'{a2}'] == 0) & (DF_result_copy[f'{b2}'] !=0), f\"{c2}\"] = 0\n",
    "    return DF_result_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ace179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.759382Z",
     "start_time": "2023-08-25T03:12:50.750301Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_DLR(statistic):\n",
    "    \"\"\"Calculate the mean SDPR value per gene promoter\"\"\"\n",
    "    statistic_group1 = statistic.groupby(by=[\"Pid\"])[\"DLR_h1\"]\n",
    "\n",
    "    groupdict1 = {}\n",
    "    for g in statistic_group1:\n",
    "        key = g[0]\n",
    "        groupdict1[key] = [ g[1].mean(), g[1].std(), len(g[1].dropna()) ]\n",
    "\n",
    "    statistic_group2 = statistic.groupby(by=[\"Pid\"])[\"DLR_h2\"]\n",
    "\n",
    "    groupdict2 = {}\n",
    "    for g in statistic_group2:\n",
    "        key = g[0]\n",
    "        groupdict2[key] = [ g[1].mean(), g[1].std(), len(g[1].dropna()) ]\n",
    "    h1_statistic = pd.DataFrame(groupdict1, index = ['h1_mean', 'h1_std', 'h1_nobs']).T\n",
    "    h2_statistic = pd.DataFrame(groupdict2, index = ['h2_mean', 'h2_std', 'h2_nobs']).T\n",
    "    \n",
    "    ttest_df = pd.merge(h1_statistic.reset_index(), h2_statistic.reset_index(), on = \"index\")\n",
    "    \n",
    "    return ttest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f38be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T03:12:50.947737Z",
     "start_time": "2023-08-25T03:12:50.934862Z"
    }
   },
   "outputs": [],
   "source": [
    "chrom_size = pd.read_csv(f\"{Rawdir}/pre_data/hg38.chromosomes.size\",\n",
    "                        sep = \"\\t\", header = None,\n",
    "                        names = ['chrom', 'size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2d7f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-25T03:13:43.606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = f\"{Rawdir}/fhap_list\"\n",
    "OUTPATH = f\"{Rawdir}/SDPR\"\n",
    "for file in chr_list:\n",
    "    #chromosome size\n",
    "    SIZE = chrom_size.loc[chrom_size.chrom == file, 'size'].tolist()[0]\n",
    "    region = [file, 0, SIZE]\n",
    "    \n",
    "    #reading TSS promoter\n",
    "    promoterfile = f\"{Rawdir}/pre_data/TSS_sorted.txt\"\n",
    "    pro_bed =  LoadingPro(promoterfile, region)#, binsize\n",
    "    \n",
    "    #loading\n",
    "    path = PATH + '/' + file\n",
    "    print(f\"Loading {path}\")\n",
    "    read_df, fhap = load_fhap(path)\n",
    "    break\n",
    "    \n",
    "    fhap_filter = filter_hp(read_df, fhap, 500)\n",
    "    fhap_filter['chrom'] = file\n",
    "    fhap_filter = fhap_filter[['chrom', 'start', 'end', 'rid', 'fid', 'cid', 'pos', 'hp', 'HType',  ]]\n",
    "    int_df = inter(pro_bed, fhap_filter)\n",
    "    rid_group = fhap_filter.groupby(\"rid\")\n",
    "    rid_dict = {}\n",
    "    for rid, df in rid_group:\n",
    "        rid_dict[rid] = df.reset_index(drop = True)\n",
    "\n",
    "    DF = file_tackle(int_df)\n",
    "    break\n",
    "\n",
    "    DF_statistic = computing_dlr(DF)\n",
    "    DF_statistic = math_div_plus(DF_statistic)\n",
    "    DF_statistic = math_div_negative(DF_statistic)\n",
    "    \n",
    "    #mean SDPR per gene promoter\n",
    "    ttest_df = mean_DLR(DF_statistic)\n",
    "    len1 = len(ttest_df)\n",
    "    ttest_df = ttest_df.dropna()\n",
    "    len2 = len(ttest_df)\n",
    "    print(f\"filtered {len1 - len2} nan genes\")\n",
    "    from scipy import stats\n",
    "    ttest_df[['statistic', 'pvalue']] = ttest_df.apply(lambda row: pd.Series(\n",
    "        stats.ttest_ind_from_stats(mean1=row['h1_mean'],\n",
    "                                   std1=row['h1_std'],\n",
    "                                   nobs1=row['h1_nobs'],\n",
    "                                   mean2=row['h2_mean'],\n",
    "                                   std2=row['h2_std'],\n",
    "                                   nobs2=row['h2_nobs'])), axis=1)\n",
    "\n",
    "    ttest_df[\"Fold_change\"] = np.log2( ttest_df[\"h1_mean\"].values /  ttest_df[\"h2_mean\"].values )\n",
    "    \n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    # alpha = 0.1\n",
    "    rejected, p_adjusted, _, alpha_corrected =  multipletests(ttest_df[\"pvalue\"].values, \n",
    "                                                              # alpha=alpha, \n",
    "                                                              method='fdr_bh', #fdr_bh\n",
    "                                                              is_sorted=False, \n",
    "                                                              returnsorted=False)\n",
    "    ttest_df[\"adjpval\"] =  p_adjusted\n",
    "    \n",
    "    print(f\"writing to {OUTPATH}/{file}/DF_mapq5.txt\")\n",
    "    DF.to_csv(OUTPATH + '/' + file + '/' + 'DF_mapq5.txt',\n",
    "             sep = \"\\t\",index = None)\n",
    "    print(f\"writing to {OUTPATH}/{file}/ttest_df_parallel_mapq5.txt\")\n",
    "    ttest_df.to_csv(OUTPATH + '/' + file + '/' + 'ttest_df_parallel_mapq5.txt',\n",
    "                     sep = \"\\t\",index = None)\n",
    "    \n",
    "\n",
    "\n",
    "    read_df = None\n",
    "    fhap = None\n",
    "    fhap_filter = None\n",
    "    int_df_filter = None\n",
    "    rid_group = None\n",
    "    rid_dict = None\n",
    "    DF = None\n",
    "    DF_statistic = None\n",
    "    pro_bed = None\n",
    "    DF_statistic = None\n",
    "    ttest_df = None\n",
    "\n",
    "    print(\"write done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e76aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
